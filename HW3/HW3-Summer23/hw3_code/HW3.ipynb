{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b32ba0fd",
   "metadata": {
    "id": "b32ba0fd"
   },
   "source": [
    "# Summer 2023 Applied NLP Homework 3\n",
    "\n",
    "## Instructors: Dr. Mahdi Roozbahani and Wafa Louhichi\n",
    "\n",
    "## Deadline: Jul 7th, 11:59PM AoE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb689246",
   "metadata": {
    "id": "eb689246"
   },
   "source": [
    "## Honor Code and Assignment Deadline\n",
    "<!-- No changes needed on the below section -->\n",
    "* No unapproved extension of the deadline is allowed. Late submission will lead to 0 credit. \n",
    "\n",
    "* Discussion is encouraged on Ed as part of the Q/A. However, all assignments should be done individually.\n",
    "<font color='darkred'>\n",
    "* Plagiarism is a **serious offense**. You are responsible for completing your own work. You are not allowed to copy and paste, or paraphrase, or submit materials created or published by others, as if you created the materials. All materials submitted must be your own.</font>\n",
    "<font color='darkred'>\n",
    "* All incidents of suspected dishonesty, plagiarism, or violations of the Georgia Tech Honor Code will be subject to the institute’s Academic Integrity procedures. If we observe any (even small) similarities/plagiarisms detected by Gradescope or our TAs, **WE WILL DIRECTLY REPORT ALL CASES TO OSI**, which may, unfortunately, lead to a very harsh outcome. **Consequences can be severe, e.g., academic probation or dismissal, grade penalties, a 0 grade for assignments concerned, and prohibition from withdrawing from the class.**\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18717a84",
   "metadata": {
    "id": "18717a84"
   },
   "source": [
    "## Instructions for the assignment \n",
    "\n",
    "<!-- No changes needed on the below section -->\n",
    "- This entire assignment will be autograded through Gradescope.\n",
    "\n",
    "- We provided you different .py files and we added libraries in those files please DO NOT remove those lines and add your code after those lines. Note that these are the only allowed libraries that you can use for the homework.\n",
    "\n",
    "- You will submit your implemented .py files to the corresponding homework section on Gradescope. \n",
    "\n",
    "- You are allowed to make as many submissions until the deadline as you like. Additionally, note that the autograder tests each function separately, therefore it can serve as a useful tool to help you debug your code if you are not sure of what part of your implementation might have an issue.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3559faf",
   "metadata": {
    "id": "f3559faf"
   },
   "source": [
    "# Google Colab Setup (Optional for running on Colab)\n",
    "You may need to right click on the Applied NLP folder and `Add shortcut to Drive`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0066960d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0066960d",
    "outputId": "ef5fd2e4-2d26-4f36-9115-3fcbbd7ec11e"
   },
   "outputs": [],
   "source": [
    "# Mount google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "\n",
    "## Change path to directory of where notebook is located\n",
    "%cd '/content/drive/MyDrive/Applied_NLP/HW3/hw3_code/'\n",
    "\n",
    "## If no GPU selected it will ask for GPU to be selected\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
    "  print('and then re-execute this cell.')\n",
    "else:\n",
    "  print(gpu_info)\n",
    "\n",
    "\n",
    "## This wraps output text according to the window size\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05409a58",
   "metadata": {
    "id": "05409a58"
   },
   "source": [
    "# Assignment Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d96973",
   "metadata": {
    "id": "89d96973"
   },
   "source": [
    "In this homework we will explore non-linear text classification algorithms : Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). We will also look into another embedding techique : Word2Vec.\n",
    "\n",
    "We will reuse the datasets from HW2 for this exploration:\n",
    "* The first dataset is a subset of a [Clickbait Dataset](https://github.com/bhargaviparanjape/clickbait/tree/master/dataset) that has article headlines and a binary label on whether the headline is considered clickbait. \n",
    "* The second dataset is a subset of [Web of Science Dataset](https://data.mendeley.com/datasets/9rw3vkcfy4/6) that has articles and a corresponding label on the domain of the articles. \n",
    "\n",
    "We will first explore Continuous Bag-of-Words (CBOW) and Skip-gram based Word2Vec models using a very small dataset. We will then use pre-trained Word2Vec embeddings and feed them to the classification algorithms.\n",
    "\n",
    "**You will be using pytorch for coding the models in this homework.** [Tutorial](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html) and [Building model](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html) are good references for those who are new to pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51f68d8",
   "metadata": {
    "id": "e51f68d8"
   },
   "source": [
    "## Deliverables and Points Distribution\n",
    "\n",
    "### Q1: Word2Vec [47pts]\n",
    "- **1.1 Implementing CBOW from Scratch** [27pts] Deliverables: <font color = 'green'>word2vec.py</font>\n",
    "\n",
    "    - [2pts] tokenize (Word2Vec class)\n",
    "\n",
    "    - [5pts] create_vocabulary (Word2Vec class)\n",
    "\n",
    "    - [10pts] cbow_embeddings (Word2Vec class)\n",
    "\n",
    "    - [5pts] \\__init__ (CBOW_Model class)\n",
    "    \n",
    "    - [5pts] forward (CBOW_Model class)\n",
    "\n",
    "- **1.2 Implementing Skip-Gram from Scratch** [20pts] Deliverables: <font color = 'green'>word2vec.py</font>\n",
    "\n",
    "    - [10pts] skipgram_embeddings (Word2Vec class)\n",
    "\n",
    "    - [5pts] \\__init__ (SkipGram_Model class)\n",
    "\n",
    "    - [5pts] forward (SkipGram_Model class)\n",
    "\n",
    "### Q2: Classification with CNN [15pts]\n",
    "- **2.1 Classification with CNN** [15pts] Deliverables: <font color = 'green'>cnn.py</font>\n",
    "\n",
    "    - [5pts] \\__init__\n",
    "\n",
    "    - [10pts] forward\n",
    "\n",
    "### Q3: Classification with RNN [15pts]\n",
    "- **3.1 Classification with RNN** [15pts] Deliverables: <font color = 'green'>rnn.py</font>\n",
    "\n",
    "    - [5pts] \\__init__ \n",
    "\n",
    "    - [10pts] forward \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f462ddb9",
   "metadata": {
    "id": "f462ddb9"
   },
   "source": [
    "# Setup\n",
    "This notebook is tested under [python 3. * . *](https://www.python.org/downloads/release/python-368/), and the corresponding packages can be downloaded from [miniconda](https://docs.conda.io/en/latest/miniconda.html). You may also want to get yourself familiar with several packages:\n",
    "\n",
    "- [jupyter notebook](https://jupyter-notebook.readthedocs.io/en/stable/)\n",
    "- [numpy](https://docs.scipy.org/doc/numpy-1.15.1/user/quickstart.html)\n",
    "- [sklearn](https://matplotlib.org/users/pyplot_tutorial.html)\n",
    "- [pytorch](https://pytorch.org/)\n",
    "\n",
    "In the .py files please implement the functions that have `raise NotImplementedError`, and after you finish the coding, please delete or comment out `raise NotImplementedError`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f056b4ff",
   "metadata": {
    "id": "f056b4ff"
   },
   "source": [
    "## Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73b47510",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 124
    },
    "id": "73b47510",
    "outputId": "28d7ed83-92ab-4951-de7e-79679cc41ee6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version information\n",
      "python: 3.11.3 (main, Apr 19 2023, 18:51:09) [Clang 14.0.6 ]\n",
      "numpy: 1.25.0\n"
     ]
    }
   ],
   "source": [
    "#Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import sys\n",
    "import re\n",
    "from copy import deepcopy\n",
    "import pickle\n",
    "import random\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "import torchtext\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "torch.manual_seed(10)\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# import gzip\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "\n",
    "print('Version information')\n",
    "\n",
    "print('python: {}'.format(sys.version))\n",
    "print('numpy: {}'.format(np.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff26cb3",
   "metadata": {
    "id": "eff26cb3"
   },
   "source": [
    "# Load Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837cc1e5",
   "metadata": {
    "id": "837cc1e5"
   },
   "source": [
    "We start by loading both data sets already split into an 80/20 train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2b97025",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "e2b97025",
    "outputId": "391ea7e2-f53d-40d4-cd91-d7989f239c7e"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "df_train = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "\n",
    "# Separate dataframes into train and test lists\n",
    "x_train, y_train = list(df_train['headline']), list(df_train['label'])\n",
    "x_test, y_test = list(df_test['headline']), list(df_test['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f2a21c",
   "metadata": {
    "id": "15f2a21c"
   },
   "source": [
    "Below is the number of headlines in the train and test set as well as a sample of the article headlines and its binary label, where 0 is considered not clickbait and 1 is clickbait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11e3ebf5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "11e3ebf5",
    "outputId": "97c351fe-624d-4b6c-b8e2-1cf03ad30dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Train Headlines: 19200\n",
      "Number of Test Headlines: 4800\n",
      "\n",
      "\n",
      "Sample Label and Headlines:\n",
      "1: 27 Breathtaking Alternatives To A Traditional Wedding Bouquet <br>\n",
      "\n",
      "1: 22 Pictures People Who Aren't Grad Students Will <strong>Never</strong> Understand\n",
      "\n",
      "0: PepsiCo Profit Falls 43 Percent\n",
      "\n",
      "0: Website of Bill O'Reilly, FOX News commentator, hacked in retribution\n",
      "\n",
      "1: The Green Toy Soldiers From Your Childhood Now Come In Baller Yoga Poses A\n",
      "\n",
      "\n",
      "Output of Sample Headlines without Print Statement:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['27 Breathtaking Alternatives To A Traditional Wedding Bouquet <br>\\n',\n",
       " \"22 Pictures People Who Aren't Grad Students Will <strong>Never</strong> Understand\\n\",\n",
       " 'PepsiCo Profit Falls 43 Percent\\n',\n",
       " \"Website of Bill O'Reilly, FOX News commentator, hacked in retribution\\n\",\n",
       " 'The Green Toy Soldiers From Your Childhood Now Come In Baller Yoga Poses A\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "print(f'Number of Train Headlines: {len(x_train)}')\n",
    "print(f'Number of Test Headlines: {len(x_test)}')\n",
    "\n",
    "print('\\n\\nSample Label and Headlines:')\n",
    "x = 105\n",
    "for label, line in zip(y_train[x:x+5], x_train[x:x+5]):\n",
    "    print(f'{label}: {line}')\n",
    "    \n",
    "print('\\nOutput of Sample Headlines without Print Statement:')\n",
    "x_train[x:x+5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20a8debc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "20a8debc",
    "outputId": "2d934897-8f6f-49bb-a85c-d627017dd8b0"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "df_train_wos = pd.read_csv('./data/train_wos.csv')\n",
    "df_test_wos = pd.read_csv('./data/test_wos.csv')\n",
    "\n",
    "# Separate dataframes into train and test lists\n",
    "x_train_wos, y_train_wos = list(df_train_wos['article']), list(df_train_wos['label'])\n",
    "x_test_wos, y_test_wos = list(df_test_wos['article']), list(df_test_wos['label'])\n",
    "\n",
    "# Numerical label to domain mapping\n",
    "wos_label = {0:'CS', 1:'ECE', 2:'Civil', 3:'Medical'}\n",
    "# Numerical label to Numerical mapping\n",
    "label_mapping = {0:0, 1:1, 4:2, 5:3}\n",
    "\n",
    "for i, label in enumerate(y_train_wos):\n",
    "    y_train_wos[i] = label_mapping[label]\n",
    "for i, label in enumerate(y_test_wos):\n",
    "    y_test_wos[i] = label_mapping[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d24ac5b6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 606
    },
    "id": "d24ac5b6",
    "outputId": "f5df643b-a8d5-41de-cba7-eede9bbaa59c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Train Articles: 1600\n",
      "Number of Test Articles: 400\n",
      "\n",
      "Label Key: {0: 'CS', 1: 'ECE', 2: 'Civil', 3: 'Medical'}\n",
      "\n",
      "Sample Label and Articles:\n",
      "\n",
      "0 - CS: An efficient procedure for calculating the electromagnetic fields in multilayered cylindrical structures is reported in this paper. Using symbolic computation, spectral Green's functions, suitable for numerical implementations are determined in compact and closed forms. Applications are presented for structures with two dielectric layers.\n",
      "\n",
      "1 - ECE: A multifunctional platform based on the microhotplate was developed for applications including a Pirani vacuum gauge, temperature, and gas sensor. It consisted of a tungsten microhotplate and an on-chip operational amplifier. The platform was fabricated in a standard complementary metal oxide semiconductor (CMOS) process. A tungsten plug in standard CMOS process was specially designed as the serpentine resistor for the microhotplate, acting as both heater and thermister. With the sacrificial layer technology, the microhotplate was suspended over the silicon substrate with a 340 nm gap. The on-chip operational amplifier provided a bias current for the microhotplate. This platform has been used to develop different kinds of sensors. The first one was a Pirani vacuum gauge ranging from 10(-1) to 10(5) Pa. The second one was a temperature sensor ranging from -20 to 70 degrees C. The third one was a thermal-conductivity gas sensor, which could distinguish gases with different thermal conductivities in constant gas pressure and environment temperature. In the fourth application, with extra fabrication processes including the deposition of gas-sensitive film, the platform was used as a metal-oxide gas sensor for the detection of gas concentration.\n",
      "\n",
      "2 - Civil: Artificial neural networks have been effectively used in various civil engineering fields, including construction management and labour productivity. In this study, the performance of the feed forward neural network (FFNN) was compared with radial basis neural network (RBNN) in modelling the productivity of masonry crews. A variety of input factors were incorporated and analysed. Mean absolute percentage error (MAPE) and correlation coefficient (R) were used to evaluate model performance. Research results indicated that the neural computing techniques could be successfully employed in modelling crew productivity. It was also found that successful models could be developed with different combinations of input factors, and several of the models which excluded one or more input factors turned out to be better than the baseline models. Based on the MAPE values obtained for the models, the RBNN technique was found to be better than the FFNN technique, although both slightly overestimated the masons' productivity.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "print(f'Number of Train Articles: {len(x_train_wos)}')\n",
    "print(f'Number of Test Articles: {len(x_test_wos)}')\n",
    "\n",
    "print('\\nLabel Key:', wos_label)\n",
    "\n",
    "print('\\nSample Label and Articles:\\n')\n",
    "x = 107\n",
    "for label, line in zip(y_train_wos[x:x+3], x_train_wos[x:x+3]):\n",
    "    print(f'{label} - {wos_label[label]}: {line}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7M6Npz6qFN1q",
   "metadata": {
    "id": "7M6Npz6qFN1q"
   },
   "source": [
    "## Q1: Word2Vec [47pts]\n",
    "\n",
    "Word2vec is a method to efficiently create word embeddings. More details on word2vec and the intuition behind it can be found here :  \n",
    "* [The Illustrated Word2vec by Jay Alammar](https://jalammar.github.io/illustrated-word2vec/)\n",
    "\n",
    "Word2vec is based on the idea that a word’s meaning is defined by its context. Context is represented as surrounding words. For the word2vec model, context is represented as N words before and N words after the current word. N is a hyperparameter. With larger N we can create better embeddings, but at the same time, such a model requires more computational resources. \n",
    "\n",
    "There are two word2vec architectures proposed in the paper:\n",
    "\n",
    "* CBOW (Continuous Bag-of-Words) — a model that predicts a current word based on its context words.\n",
    "* Skip-Gram — a model that predicts context words based on the current word.\n",
    "\n",
    "We will be running our Word2Vec models on a very small dataset as described below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "mC1mB6pYqP4U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "mC1mB6pYqP4U",
    "outputId": "41393f89-e928-40c9-c875-e603e36affd9"
   },
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'he is a king',\n",
    "    'she is a queen',\n",
    "    'he is a man',\n",
    "    'she is a woman',\n",
    "    'warsaw is poland capital',\n",
    "    'berlin is germany capital',\n",
    "    'paris is france capital',   \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-Np3UUmtFUjd",
   "metadata": {
    "id": "-Np3UUmtFUjd"
   },
   "source": [
    "## 1.1: Implementing Continuous Bag-of-words From Scratch [27pts]\n",
    "In the **word2vec.py** file complete the following functions:\n",
    "  * <strong>tokenize</strong> (Word2Vec class)\n",
    "  * <strong>create_vocabulary</strong> (Word2Vec class)\n",
    "  * <strong>cbow_embeddings</strong> (Word2Vec class)\n",
    "  * <strong>\\__init__</strong> (CBOW_Model class)\n",
    "  * <strong>forward</strong> (CBOW_Model class)\n",
    "\n",
    "A high level overview of the CBOW model can be described as :    \n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/1400/1*ETcgajy5s0KNIfMgE5xOqg.png\" width=\"75%\" align=\"center\"></p>\n",
    "\n",
    "CBOW model takes several words, each goes through the same Embedding layer, and then word embedding vectors are averaged before going into the Linear layer.\n",
    "\n",
    "We will be implementing this model using the architecture described below :    \n",
    "\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/1400/1*mLDM3PH12CjhaFoUm5QTow.png\" width=\"75%\" align=\"center\"></p>\n",
    "\n",
    "Here are the steps that needs to be followed for implementing CBOW model :    \n",
    "* Step-1: Create vocabulary\n",
    "  * Split each words into tokens.\n",
    "  * Assign a unique ID to each unique token.\n",
    "\n",
    "* Step-2: Create CBOW Embeddings\n",
    "  * Create CBOW embeddings by taking context as N past words and N future words.\n",
    "\n",
    "* Step-3: Implement CBOW Model\n",
    "  * Implement CBOW model as described in the architecture above.\n",
    "  \n",
    "<b>Hint:</b> Since we are using the cross entropy loss, there is no need to apply the softmax after the linear layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jd9OaAf9qZ5r",
   "metadata": {
    "id": "jd9OaAf9qZ5r"
   },
   "source": [
    "## 1.1.1: Fetching CBOW embeddings [No Points]\n",
    "Run the below cell to fetch CBOW embeddings using functions that you have already implemented in **1.1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6bd51ab7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "6bd51ab7",
    "outputId": "2ecda65b-64cc-458b-9c1c-102cb0a9e7b1"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from word2vec import Word2Vec\n",
    "\n",
    "w2v = Word2Vec()\n",
    "tokens = w2v.tokenize(corpus)\n",
    "w2v.create_vocabulary(tokens)\n",
    "source, target = w2v.cbow_embeddings(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ce54e81b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'a', 'king'], ['she', 'is', 'a', 'queen'], ['he', 'is', 'a', 'man'], ['she', 'is', 'a', 'woman'], ['warsaw', 'is', 'poland', 'capital'], ['berlin', 'is', 'germany', 'capital'], ['paris', 'is', 'france', 'capital']]\n",
      "-------------------------------\n",
      "[[6, 0], [5, 0, 7], [5, 6, 7], [6, 0], [6, 0], [12, 0, 11], [12, 6, 11], [6, 0], [6, 0], [5, 0, 8], [5, 6, 8], [6, 0], [6, 0], [12, 0, 14], [12, 6, 14], [6, 0], [6, 10], [13, 10, 2], [13, 6, 2], [6, 10], [6, 4], [1, 4, 2], [1, 6, 2], [6, 4], [6, 3], [9, 3, 2], [9, 6, 2], [6, 3]]\n",
      "-------------------------------\n",
      "[[5], [6], [0], [7], [12], [6], [0], [11], [5], [6], [0], [8], [12], [6], [0], [14], [13], [6], [10], [2], [1], [6], [4], [2], [9], [6], [3], [2]]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)\n",
    "print('-------------------------------')\n",
    "print(source)\n",
    "print('-------------------------------')\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3t3BiUU1rMM6",
   "metadata": {
    "id": "3t3BiUU1rMM6"
   },
   "source": [
    "## 1.1.2: Training CBOW model [No Points]\n",
    "Run the below cell to train CBOW model using functions that you have already implemented in **1.1**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "1fCSFxldedmf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "1fCSFxldedmf",
    "outputId": "6dc04ce3-941e-409e-8a62-e758b2ad4f3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 300])\n",
      "torch.Size([1, 15])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m#print(outputs, y)\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y)\n\u001b[1;32m     34\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[1;32m     35\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_hw3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_hw3/lib/python3.11/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_hw3/lib/python3.11/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from word2vec import Word2Vec\n",
    "\n",
    "w2v = Word2Vec()\n",
    "tokens = w2v.tokenize(corpus)\n",
    "w2v.create_vocabulary(tokens)\n",
    "source, target = w2v.cbow_embeddings(tokens)\n",
    "\n",
    "from word2vec import CBOW_Model\n",
    "\n",
    "N_EPOCHS = 300\n",
    "model = CBOW_Model(w2v.vocabulary_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    shuffled_i = list(range(0,len(target)))\n",
    "    random.shuffle(shuffled_i)\n",
    "    for i in shuffled_i:\n",
    "        x = torch.from_numpy(np.asarray(source[i])).long().to(device)\n",
    "        y = torch.from_numpy(np.asarray(target[i])).float().to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        #print(outputs, y)\n",
    "        loss = criterion(outputs, y)\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 20 == 0:    \n",
    "      print(\"loss on epoch %i: %f\" % (epoch, total_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GvdTvbADsOgr",
   "metadata": {
    "id": "GvdTvbADsOgr"
   },
   "source": [
    "## 1.1.3: Visualizing CBOW embeddings [No Points]\n",
    "Run the below cells to visualize CBOW embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pgmVkIigg1xr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "pgmVkIigg1xr",
    "outputId": "94f2ec1e-dfe6-4c7d-ab93-adbd4785d308"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# embedding from first model layer\n",
    "embeddings = list(model.parameters())[0]\n",
    "embeddings = embeddings.cpu().detach().numpy()\n",
    "\n",
    "# normalization\n",
    "norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
    "norms = np.reshape(norms, (len(norms), 1))\n",
    "embeddings_norm = embeddings / norms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce56210",
   "metadata": {},
   "source": [
    "Here, we use truncated SVD to project the learned word2vec embedding to 2D space for visualization. Feel free to tune the learning rate in the training part for different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PYcXX6AIpcun",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "PYcXX6AIpcun",
    "outputId": "6a76217b-527b-410c-ae23-e81eccd32115"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn import decomposition\n",
    "\n",
    "w2v.word2idx[''] = 0\n",
    "svd = decomposition.TruncatedSVD(n_components=2)\n",
    "W2_dec = svd.fit_transform(embeddings)\n",
    "\n",
    "x = W2_dec[:,0]\n",
    "y = W2_dec[:,1]\n",
    "plot = sns.scatterplot(x=x, y=y)\n",
    "\n",
    "for i in range(0,W2_dec.shape[0]):\n",
    "     plot.text(x[i], y[i]+2e-2, list(w2v.word2idx)[i], horizontalalignment='center', size='small', color='black', weight='semibold');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b673bc",
   "metadata": {},
   "source": [
    "Here, we will look into the learned property of the word2vec embedding. Warsaw is the capital of Poland and we now calculate the difference between embeddings of \"warsaw\" and \"poland\". After that, we add the difference to the embedding of \"paris\". We then rank the dot product of the computed embedding vs all the embeddings. Notice that the larger this value, the more similar two embeddings are. Feel free to play with different word pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3V4LYCq5mVB1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "3V4LYCq5mVB1",
    "outputId": "c652ce3a-0ca3-49f6-ec29-d1b22c0fdb6e"
   },
   "outputs": [],
   "source": [
    "emb1 = embeddings[w2v.word2idx[\"poland\"]]\n",
    "emb2 = embeddings[w2v.word2idx[\"warsaw\"]]\n",
    "emb3 = embeddings[w2v.word2idx[\"paris\"]]\n",
    "\n",
    "emb4 = emb1 - emb2 + emb3\n",
    "emb4_norm = (emb4 ** 2).sum() ** (1 / 2)\n",
    "emb4 = emb4 / emb4_norm\n",
    "\n",
    "emb4 = np.reshape(emb4, (len(emb4), 1))\n",
    "dists = np.matmul(embeddings_norm, emb4).flatten()\n",
    "\n",
    "top5 = np.argsort(-dists)[:5]\n",
    "\n",
    "for word_id in top5:\n",
    "    print(\"{}: {:.3f}\".format(w2v.idx2word[word_id], dists[word_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4TLUiFFR7wlL",
   "metadata": {
    "id": "4TLUiFFR7wlL"
   },
   "source": [
    "## 1.2: Implementing Skip-Gram From Scratch [20pts]\n",
    "In the **word2vec.py** file complete the following functions:\n",
    "  * <strong>skipgram_embeddings</strong> (Word2Vec class)\n",
    "  * <strong>\\__init__</strong> (SkipGram_Model class)\n",
    "  * <strong>forward</strong> (SkipGram_Model class)\n",
    "\n",
    "A high level overview of the SkipGram model can be described as :    \n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/720/1*SVs6xTpD7AYviP24UTOYUA.png\" width=\"75%\" align=\"center\"></p>\n",
    "\n",
    "The Skip-Gram model takes a single word as compared to CBOW model.\n",
    "\n",
    "We will be implementing this model using the architecture described below :    \n",
    "\n",
    "<p align=\"center\"><img src=\"https://miro.medium.com/max/720/1*eHh1_t8Wms_hqDNBLuAnFg.png\" width=\"75%\" align=\"center\"></p>\n",
    "\n",
    "Here are the steps that needs to be followed for implementing SkipGram model :    \n",
    "* Step-1: Create vocabulary\n",
    "  * Split each words into tokens.\n",
    "  * Assign a unique ID to each unique token.\n",
    "\n",
    "* Step-2: Create SkipGram Embeddings\n",
    "  * Create SkipGram embeddings by taking context as middle word.\n",
    "\n",
    "* Step-3: Implement SkipGram Model\n",
    "  * Implement SkipGram model as described in the architecture above. Output SkipGram embeddings for N past words and N future words.\n",
    "\n",
    "**Hint:** Since we are using the cross entropy loss, there is no need to apply the softmax after the linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IVCLHrC788LU",
   "metadata": {
    "id": "IVCLHrC788LU"
   },
   "source": [
    "## 1.2.1: Fetching SkipGram embeddings [No Points]\n",
    "Run the below cell to fetch **SkipGram** embeddings using functions that you have already implemented in 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ecu9TMvO83p9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ecu9TMvO83p9",
    "outputId": "f658bea1-dbdc-4867-d1fd-9b0bf25cc2e7"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from word2vec import Word2Vec\n",
    "\n",
    "w2v = Word2Vec()\n",
    "tokens = w2v.tokenize(corpus)\n",
    "w2v.create_vocabulary(tokens)\n",
    "source, target = w2v.skipgram_embeddings(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "7b13a2b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['he', 'is', 'a', 'king'], ['she', 'is', 'a', 'queen'], ['he', 'is', 'a', 'man'], ['she', 'is', 'a', 'woman'], ['warsaw', 'is', 'poland', 'capital'], ['berlin', 'is', 'germany', 'capital'], ['paris', 'is', 'france', 'capital']]\n",
      "[[5], [5], [6], [6], [6], [0], [0], [0], [7], [7], [12], [12], [6], [6], [6], [0], [0], [0], [11], [11], [5], [5], [6], [6], [6], [0], [0], [0], [8], [8], [12], [12], [6], [6], [6], [0], [0], [0], [14], [14], [13], [13], [6], [6], [6], [10], [10], [10], [2], [2], [1], [1], [6], [6], [6], [4], [4], [4], [2], [2], [9], [9], [6], [6], [6], [3], [3], [3], [2], [2]]\n",
      "[[6], [0], [5], [0], [7], [5], [6], [7], [6], [0], [6], [0], [12], [0], [11], [12], [6], [11], [6], [0], [6], [0], [5], [0], [8], [5], [6], [8], [6], [0], [6], [0], [12], [0], [14], [12], [6], [14], [6], [0], [6], [10], [13], [10], [2], [13], [6], [2], [6], [10], [6], [4], [1], [4], [2], [1], [6], [2], [6], [4], [6], [3], [9], [3], [2], [9], [6], [2], [6], [3]]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)\n",
    "print(source)\n",
    "print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4lU_G6iB9N9d",
   "metadata": {
    "id": "4lU_G6iB9N9d"
   },
   "source": [
    "## 1.2.2: Training SkipGram model [No Points]\n",
    "Run the below cell to train SkipGram model using functions that you have already implemented in **1.2**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "JGRhDekI83si",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "JGRhDekI83si",
    "outputId": "c7fec093-54d0-4b8a-95b0-21b5030e6fa9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 300])\n",
      "torch.Size([15])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch (got input: [15], target: [1])",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[149], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#print(outputs.shape, y.shape, outputs.dtype, y.dtype)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m#print(outputs)\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#print(y)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, y)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m#print(loss)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_hw3/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_hw3/lib/python3.11/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[0;32m~/anaconda3/envs/nlp_hw3/lib/python3.11/site-packages/torch/nn/functional.py:3029\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3028\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3029\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch (got input: [15], target: [1])"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from word2vec import SkipGram_Model\n",
    "\n",
    "N_EPOCHS = 300\n",
    "model = SkipGram_Model(w2v.vocabulary_size)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    shuffled_i = list(range(0,len(target)))\n",
    "    random.shuffle(shuffled_i)\n",
    "    for i in shuffled_i:\n",
    "        x = torch.from_numpy(np.asarray(source[i])).long().to(device)\n",
    "        y = torch.from_numpy(np.asarray(target[i])).float().to(device)\n",
    "        #x = torch.from_numpy(np.asarray(source[i])).to(device)\n",
    "        #y = torch.from_numpy(np.asarray(target[i])).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x)\n",
    "        #print(outputs.shape, y.shape, outputs.dtype, y.dtype)\n",
    "        #print(outputs)\n",
    "        #print(y)\n",
    "        loss = criterion(outputs, y)\n",
    "        #print(loss)\n",
    "        total_loss += loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    if epoch % 20 == 0:    \n",
    "      print(\"loss on epoch %i: %f\" % (epoch, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "738a2d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = torch.tensor([[ 0.0240],\n",
    "        [ 0.0240],\n",
    "        [-0.0501],\n",
    "        [-0.0501],\n",
    "        [-0.0501],\n",
    "        [-0.0310],\n",
    "        [-0.0310],\n",
    "        [-0.0310],\n",
    "        [ 0.0065],\n",
    "        [ 0.0065]])\n",
    "\n",
    "test2 = torch.tensor([[6.],\n",
    "        [4.],\n",
    "        [1.],\n",
    "        [4.],\n",
    "        [2.],\n",
    "        [1.],\n",
    "        [6.],\n",
    "        [2.],\n",
    "        [6.],\n",
    "        [4.]])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "print(test1.shape, test2.shape)\n",
    "loss = criterion(test1, test2)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HOthB5MD9ruv",
   "metadata": {
    "id": "HOthB5MD9ruv"
   },
   "source": [
    "## 1.2.3: Visualizing SkipGram embeddings [No Points]\n",
    "Run the below cells to visualize SkipGram embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MW64QQ4n83vK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "MW64QQ4n83vK",
    "outputId": "acb32f8c-6dde-4277-c519-fa9a1977c548"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# embedding from first model layer\n",
    "embeddings = list(model.parameters())[0]\n",
    "embeddings = embeddings.cpu().detach().numpy()\n",
    "\n",
    "# normalization\n",
    "norms = (embeddings ** 2).sum(axis=1) ** (1 / 2)\n",
    "norms = np.reshape(norms, (len(norms), 1))\n",
    "embeddings_norm = embeddings / norms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37862141",
   "metadata": {},
   "source": [
    "Here, we use truncated SVD to project the learned word2vec embedding to 2D space for visualization. Feel free to tune the learning rate in the training part for different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iLhTgwE183xt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265
    },
    "id": "iLhTgwE183xt",
    "outputId": "832755ed-5da8-47b4-eb4a-41d15f938d65"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from sklearn import decomposition\n",
    "\n",
    "w2v.word2idx[''] = 0\n",
    "svd = decomposition.TruncatedSVD(n_components=2)\n",
    "W2_dec = svd.fit_transform(embeddings)\n",
    "\n",
    "x = W2_dec[:,0]\n",
    "y = W2_dec[:,1]\n",
    "plot = sns.scatterplot(x=x, y=y)\n",
    "\n",
    "for i in range(0,W2_dec.shape[0]):\n",
    "     plot.text(x[i], y[i]+2e-2, list(w2v.word2idx)[i], horizontalalignment='center', size='small', color='black', weight='semibold');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19a6dd6",
   "metadata": {},
   "source": [
    "Here, we will look into the learned property of the word2vec embedding. Warsaw is the capital of Poland and we now calculate the difference between embeddings of \"warsaw\" and \"poland\". After that, we add the difference to the embedding of \"paris\". We then rank the dot product of the computed embedding vs all the embeddings. Notice that the larger this value, the more similar two embeddings are. Feel free to play with different word pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6Opu7Awy979w",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "6Opu7Awy979w",
    "outputId": "058539e5-171d-4c34-b086-5a283fc82684"
   },
   "outputs": [],
   "source": [
    "emb1 = embeddings[w2v.word2idx[\"poland\"]]\n",
    "emb2 = embeddings[w2v.word2idx[\"warsaw\"]]\n",
    "emb3 = embeddings[w2v.word2idx[\"paris\"]]\n",
    "\n",
    "emb4 = emb1 - emb2 + emb3\n",
    "emb4_norm = (emb4 ** 2).sum() ** (1 / 2)\n",
    "emb4 = emb4 / emb4_norm\n",
    "\n",
    "emb4 = np.reshape(emb4, (len(emb4), 1))\n",
    "dists = np.matmul(embeddings_norm, emb4).flatten()\n",
    "\n",
    "top5 = np.argsort(-dists)[:5]\n",
    "\n",
    "for word_id in top5:\n",
    "    print(\"{}: {:.3f}\".format(w2v.idx2word[word_id], dists[word_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DqJwpMAYUg4A",
   "metadata": {
    "id": "DqJwpMAYUg4A"
   },
   "source": [
    "Ideally with large amount of data, we should have got france as the most nearest word. But depending on the implementation and amount of data, it can vary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dRvybL9rYxcQ",
   "metadata": {
    "id": "dRvybL9rYxcQ"
   },
   "source": [
    "## Q2: Classification with CNN [15pts]\n",
    "\n",
    "Convolutional layers are used to find patterns by sliding small kernel window over input. Instead of multiplying the filters on the small regions of the images, it slides through embedding vectors of few words as mentioned by window size. For looking at sequences of word embeddings, the window has to look at multiple word embeddings in a sequence. They will be rectangular with size window_size * embedding_size. For example, if window size is 3 then kernel will be 3*500. This essentially represents n-grams in the model. The kernel weights (filter) are multiplied to word embeddings in pairs and summed up to get output values. As the network is being learned, these kernel weights are also being learned.\n",
    "\n",
    "We will be using convolutional network with pre-trained word2vec models for classification. We implement a convolutional neural network for text classification similar to the CNN-rand baseline described by [Kim (2014)](https://aclanthology.org/D14-1181.pdf). We use pre-trained word2vec models for feasibility of finding appropriate embeddings. The architecture of our model looks like :\n",
    "\n",
    "<p align=\"center\"><img src=\"https://cezannec.github.io/assets/cnn_text/complete_text_classification_CNN.png\" width=\"75%\" align=\"center\"></p>\n",
    "\n",
    "We will be using an Embedding layer loaded with a word2vec model, followed by a convolution layer, and a linear layer.\n",
    "\n",
    "We will would be using the Clickbait and Web of science dataset for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5hBTTf_0bDWd",
   "metadata": {
    "id": "5hBTTf_0bDWd"
   },
   "source": [
    "## 2.1: Implementing CNN classifier\n",
    "In the **cnn.py** file complete the following functions:\n",
    "  * <strong>\\__init__</strong>\n",
    "  * <strong>forward</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BlQWYkt6bjPM",
   "metadata": {
    "id": "BlQWYkt6bjPM"
   },
   "source": [
    "### 2.1.1 : Pre-Processing Data [No Points]\n",
    "\n",
    "Run the below cell to preprocess the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "QtXq7iy3bwrs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "QtXq7iy3bwrs",
    "outputId": "e4849779-987a-4a07-d1bb-c4cb867814bc"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def preprocess(data):\n",
    "  preprocessed_data = []\n",
    "  for text in data:\n",
    "    tokens = simple_preprocess(text, deacc=True)\n",
    "    preprocessed_data.append(tokens)\n",
    "  return preprocessed_data\n",
    "\n",
    "preprocessed_x_train = preprocess(x_train)\n",
    "preprocessed_x_train_wos = preprocess(x_train_wos)\n",
    "\n",
    "preprocessed_x_test = preprocess(x_test)\n",
    "preprocessed_x_test_wos = preprocess(x_test_wos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "_M0alpCXcN3N",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "id": "_M0alpCXcN3N",
    "outputId": "6ffd3641-ff5e-4366-958d-f18399a31bd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for running: \n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Use cuda if present\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device available for running: \")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vOr_wpjNdGnj",
   "metadata": {
    "id": "vOr_wpjNdGnj"
   },
   "source": [
    "### 2.1.2 : Utility functions for training Word2Vec Model [No Points]\n",
    "\n",
    "Run the below cells for making word2vec model, vectors and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3gjLdwNOdOYd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "3gjLdwNOdOYd",
    "outputId": "2315178e-7fd3-42bd-c555-8af5915f7976"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "size = 500\n",
    "window = 3\n",
    "min_count = 1\n",
    "workers = 3\n",
    "sg = 1\n",
    "\n",
    "# Function to train word2vec model\n",
    "def make_word2vec_model(data, padding=True, sg=1, min_count=1, vector_size=500, workers=3, window=3):\n",
    "    data.append(['pad'])\n",
    "    w2v_model = Word2Vec(data, min_count = min_count, vector_size = size, workers = workers, window = window, sg = sg)\n",
    "    return w2v_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "DTUjqQnjdOUz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "DTUjqQnjdOUz",
    "outputId": "11051581-803c-477e-9788-af3d12a9e201"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "def make_word2vec_vector(sentence):\n",
    "    padded_X = [padding_idx for i in range(max_sen_len)]\n",
    "    i = 0\n",
    "    for word in sentence:\n",
    "        #if word not in w2vmodel.wv.vocab:\n",
    "        if word not in w2vmodel.wv.index_to_key:\n",
    "            padded_X[i] = 0\n",
    "        else:\n",
    "            padded_X[i] = w2vmodel.wv.key_to_index[word]\n",
    "        i += 1\n",
    "    return torch.tensor(padded_X, dtype=torch.long, device=device).view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "PrOTzz5gddgj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "PrOTzz5gddgj",
    "outputId": "70681d17-e12c-44bd-8be2-fd7be0c7b8c9"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "def make_target(label):\n",
    "  return torch.tensor([label], dtype=torch.long, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fXuJ6ouncC8Z",
   "metadata": {
    "id": "fXuJ6ouncC8Z"
   },
   "source": [
    "## 2.2 : Classifying Clickbait Dataset using CNN [No Points]\n",
    "\n",
    "Run the below cell to classify the Clickbait train and test dataset using the CNN functions that you have already implemented in 2.\n",
    "\n",
    "An accuracy of more than 80% is acceptable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "lurnaUoUXCcU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "lurnaUoUXCcU",
    "outputId": "dece0006-ceab-4123-cb16-7e728f2e6f3c"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Train Word2vec model\n",
    "w2vmodel = make_word2vec_model(preprocessed_x_train, padding=True, sg=sg, min_count=min_count, vector_size=size, workers=workers, window=window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6a6051b",
   "metadata": {},
   "source": [
    "Because CNN requires the input data to be of the same length. We use the embedding of the \"pad\" word as the padding vector. Notice that this choice is just a convention and other tokens could also work for this purpose. In more complex language model, there will be a dedicated '\\<pad\\>' token for padding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ba40fd00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.3 (main, Apr 19 2023, 18:51:09) [Clang 14.0.6 ]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3fcf871a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim==3.8.0 in /Users/nickdinapoli/anaconda3/lib/python3.10/site-packages (3.8.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/nickdinapoli/anaconda3/lib/python3.10/site-packages (from gensim==3.8.0) (1.10.0)\n",
      "Requirement already satisfied: smart-open>=1.7.0 in /Users/nickdinapoli/anaconda3/lib/python3.10/site-packages (from gensim==3.8.0) (5.2.1)\n",
      "Requirement already satisfied: six>=1.5.0 in /Users/nickdinapoli/anaconda3/lib/python3.10/site-packages (from gensim==3.8.0) (1.16.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /Users/nickdinapoli/anaconda3/lib/python3.10/site-packages (from gensim==3.8.0) (1.23.5)\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "#!pip install python==3.7.0\n",
    "!pip install gensim==3.8.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b1faad5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: gensim\r\n",
      "Version: 3.8.0\r\n",
      "Summary: Python framework for fast Vector Space Modelling\r\n",
      "Home-page: http://radimrehurek.com/gensim\r\n",
      "Author: Radim Rehurek\r\n",
      "Author-email: me@radimrehurek.com\r\n",
      "License: LGPLv2.1\r\n",
      "Location: /Users/nickdinapoli/anaconda3/lib/python3.10/site-packages\r\n",
      "Requires: numpy, scipy, six, smart-open\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "!pip show gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a27a872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#w2vmodel.wv.index_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "MnRN7kn7XCe6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "MnRN7kn7XCe6",
    "outputId": "39df6412-550c-4e4c-c892-3473877470da"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "max_sen_len = max(map(len, preprocessed_x_train))\n",
    "padding_idx = w2vmodel.wv.key_to_index['pad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ca3215f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 106
    },
    "id": "zIdO6zAsUMGV",
    "outputId": "d7cce669-26ac-45cc-e11c-827cfd122925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 0: 7284.202690\n",
      "loss on epoch 1: 7150.303315\n",
      "loss on epoch 2: 7104.886764\n",
      "loss on epoch 3: 7075.922083\n",
      "loss on epoch 4: 7064.057964\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from cnn import CNN\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "model = CNN(w2vmodel, num_classes=NUM_CLASSES)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "N_EPOCHS = 5\n",
    "\n",
    "model.train()\n",
    "for epoch in range(N_EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    shuffled_i = list(range(0,len(y_train)))\n",
    "    random.shuffle(shuffled_i)\n",
    "\n",
    "    for index in range(len(shuffled_i)):\n",
    "        model.zero_grad()\n",
    "        bow_vec = make_word2vec_vector(preprocessed_x_train[index]).float()\n",
    "        outputs = model(bow_vec)\n",
    "        y = make_target(y_train[index])\n",
    "\n",
    "        loss = criterion(outputs, y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    print(\"loss on epoch %i: %f\" % (epoch, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aOiTWeR6VDXk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "aOiTWeR6VDXk",
    "outputId": "b02e1d0d-5194-4b47-faec-cdba1a3b330f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy on Clickbait Dataset using CNN : 0.939\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "cnn_predictions = []\n",
    "original_lables_cnn = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index in range(len(y_test)):\n",
    "        bow_vec = make_word2vec_vector(preprocessed_x_test[index])\n",
    "        probs = model(bow_vec)\n",
    "        _, predicted = torch.max(probs.data, 1)\n",
    "        cnn_predictions.append(predicted.cpu().numpy()[0])\n",
    "        t = make_target(y_test[index]).cpu().numpy()[0]\n",
    "        original_lables_cnn.append(make_target(y_test[index]).cpu().numpy()[0])\n",
    "\n",
    "print(\"Test Accuracy on Clickbait Dataset using CNN : {:.3f}\".format(accuracy_score(original_lables_cnn, cnn_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6XdylnuwxUH",
   "metadata": {
    "id": "a6XdylnuwxUH"
   },
   "source": [
    "Run the below cell to save the predictions. You will be required to upload the predictions on gradescope for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "Z1di5i2NwyXj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "Z1di5i2NwyXj",
    "outputId": "0280d9bb-cb4f-4f8b-ed85-606c5ab97b31"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "preds = np.asarray(cnn_predictions)\n",
    "\n",
    "with open('cnn_clickbait.pkl', 'wb') as fp:\n",
    "    pickle.dump(preds, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lRXO38UrmQ18",
   "metadata": {
    "id": "lRXO38UrmQ18"
   },
   "source": [
    "## 2.3 : Classifying Web of Science Dataset using CNN [No Points]\n",
    "\n",
    "Run the below cell to classify the WoS train and test dataset using the CNN functions that you have already implemented in 2.\n",
    "\n",
    "An accuracy of more than 55% is acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "XUyZvrX3mWXj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "XUyZvrX3mWXj",
    "outputId": "90050d4c-c393-483d-c76d-d7cc475e3ce7"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "# Train Word2vec model\n",
    "w2vmodel = make_word2vec_model(preprocessed_x_train_wos, padding=True, sg=sg, min_count=min_count, vector_size=size, workers=workers, window=window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "dP07m73fRM44",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "dP07m73fRM44",
    "outputId": "28f17390-69d5-4178-f8ec-9a51ae9dbaa6"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "max_sen_len = max(map(len, preprocessed_x_train_wos))\n",
    "padding_idx = w2vmodel.wv.key_to_index['pad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "HZPVaPsvmoDR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "HZPVaPsvmoDR",
    "outputId": "21294fa2-48a5-455e-c7fa-fd8d5d9c602f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 0: 1969.677522\n",
      "loss on epoch 5: 1548.603587\n",
      "loss on epoch 10: 1461.306056\n",
      "loss on epoch 15: 1408.127949\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from cnn import CNN\n",
    "\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "model = CNN(w2vmodel, num_classes=NUM_CLASSES)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "N_EPOCHS = 20\n",
    "\n",
    "model.train()\n",
    "for epoch in range(N_EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    shuffled_i = list(range(0,len(y_train_wos)))\n",
    "    random.shuffle(shuffled_i)\n",
    "\n",
    "    for index in range(len(shuffled_i)):\n",
    "        model.zero_grad()\n",
    "        bow_vec = make_word2vec_vector(preprocessed_x_train_wos[index]).float()\n",
    "        outputs = model(bow_vec)\n",
    "        y = make_target(y_train_wos[index])\n",
    "\n",
    "        loss = criterion(outputs, y)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    if epoch % 5 == 0:    \n",
    "      print(\"loss on epoch %i: %f\" % (epoch, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "E3bSg3HCYnIS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "E3bSg3HCYnIS",
    "outputId": "e47cc8c4-3846-46bc-f7b2-7120947e8263"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy on WoS Dataset using CNN : 0.640\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "cnn_predictions = []\n",
    "original_lables_cnn = []\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for index in range(len(y_test_wos)):\n",
    "        bow_vec = make_word2vec_vector(preprocessed_x_test_wos[index])\n",
    "        probs = model(bow_vec)\n",
    "        _, predicted = torch.max(probs.data, 1)\n",
    "        cnn_predictions.append(predicted.cpu().numpy()[0])\n",
    "        t = make_target(y_test_wos[index]).cpu().numpy()[0]\n",
    "        original_lables_cnn.append(make_target(y_test_wos[index]).cpu().numpy()[0])\n",
    "\n",
    "print(\"Test Accuracy on WoS Dataset using CNN : {:.3f}\".format(accuracy_score(original_lables_cnn, cnn_predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "px4Og3agxxkY",
   "metadata": {
    "id": "px4Og3agxxkY"
   },
   "source": [
    "Run the below cell to save the predictions. You will be required to upload the predictions on gradescope for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "XwwCsXxux2nS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "XwwCsXxux2nS",
    "outputId": "7b88e6a6-9122-4ed7-a223-c7892856f9b7"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "preds = np.asarray(cnn_predictions)\n",
    "\n",
    "with open('cnn_wos.pkl', 'wb') as fp:\n",
    "    pickle.dump(preds, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hJaO3vd_oP-Z",
   "metadata": {
    "id": "hJaO3vd_oP-Z"
   },
   "source": [
    "## Q3: Classification with RNN [15pts]\n",
    "\n",
    "\n",
    "We will be using recurrent neural networks for classification. The architecture of our model looks like :\n",
    "\n",
    "<p align=\"center\"><img src=\"https://www.tensorflow.org/static/text/tutorials/images/bidirectional.png\" width=\"75%\" align=\"center\"></p>\n",
    "\n",
    "We will be using an Embedding layer loaded, followed by a RNN layer, and a linear layer.\n",
    "\n",
    "We will would be using the Clickbait and Web of science dataset for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alo7oi32yrwp",
   "metadata": {
    "id": "alo7oi32yrwp"
   },
   "source": [
    "## 3.1: Implementing RNN classifier\n",
    "In the **rnn.py** file complete the following functions:\n",
    "  * <strong>\\__init__</strong>\n",
    "  * <strong>forward</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Aajl3628zAkp",
   "metadata": {
    "id": "Aajl3628zAkp"
   },
   "source": [
    "### 3.1.1 : Pre-Processing Data [No Points]\n",
    "\n",
    "Run the below cells to load functions for building vocabulary and tokenizing the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8qk1qpnIzCwe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "8qk1qpnIzCwe",
    "outputId": "5f75fc50-8aaa-4a09-dc2e-f78bcc8ed8d3"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def build_vocabulary(datasets):\n",
    "  for dataset in datasets:\n",
    "    for text in dataset:\n",
    "      yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(build_vocabulary([x_train]), min_freq=1, specials=[\"<UNK>\"])\n",
    "vocab.set_default_index(vocab[\"<UNK>\"])\n",
    "\n",
    "vocab_wos = build_vocab_from_iterator(build_vocabulary([x_train_wos]), min_freq=1, specials=[\"<UNK>\"])\n",
    "vocab_wos.set_default_index(vocab[\"<UNK>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EUMc-XzSvVVM",
   "metadata": {
    "id": "EUMc-XzSvVVM"
   },
   "source": [
    "## 3.2 : Classifying Clickbait Dataset using RNN [No Points]\n",
    "\n",
    "Run the below cell to classify the Clickbait train and test dataset using the RNN functions that you have already implemented in 3.\n",
    "\n",
    "An accuracy of more than 85% is acceptable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "JGth1KFuzKMe",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "JGth1KFuzKMe",
    "outputId": "70e6c739-3d64-4751-b2dd-801931eb967f"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "max_words = 0\n",
    "for t in x_train:\n",
    "    max_words = max(max_words, len(vocab_wos(tokenizer(t))))\n",
    "\n",
    "def vectorize_batch(batch):\n",
    "    Y, X = list(zip(*batch))\n",
    "    X = [vocab(tokenizer(text)) for text in X] ## Tokenize and map tokens to indexes\n",
    "    X_len = [len(text) for text in X]\n",
    "    X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] ## Bringing all samples to max_words length.\n",
    "    return torch.tensor(X, dtype=torch.int32), torch.tensor(X_len), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "bPd8FJ2ozNsf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "bPd8FJ2ozNsf",
    "outputId": "b1bad498-5727-46ed-ae3a-3f2f2d4a7668"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "train_dataset = list(map(lambda y, x: (y, x), y_train, x_train))\n",
    "test_dataset = list(map(lambda y, x: (y, x), y_test, x_test))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1024, collate_fn=vectorize_batch, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1024, collate_fn=vectorize_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "8351afd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18559\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "zlXccqKokN-u",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "id": "zlXccqKokN-u",
    "outputId": "bc849b0f-43ad-4783-f8ad-da88656026d0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 19/19 [00:06<00:00,  3.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 0: 7.789461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 19/19 [00:06<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 1: 2.756296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 19/19 [00:06<00:00,  3.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 2: 1.257725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 19/19 [00:06<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 3: 0.584716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 19/19 [00:06<00:00,  2.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 4: 0.307369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from rnn import RNN\n",
    "from tqdm import tqdm\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "\n",
    "model = RNN(vocab, num_classes=NUM_CLASSES)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "N_EPOCHS = 5\n",
    "\n",
    "model.train()\n",
    "for epoch in range(N_EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    for X, X_len, Y in tqdm(train_loader):\n",
    "      X = X.to(device)\n",
    "      Y = Y.to(device)\n",
    "      outputs = model(X, X_len)\n",
    "      loss = criterion(outputs, Y)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      total_loss += loss.item()\n",
    "\n",
    "    print(\"loss on epoch %i: %f\" % (epoch, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "UjopZ85QsEcV",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "UjopZ85QsEcV",
    "outputId": "bac143d6-28f5-4f83-a18f-b00176d82080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1024, 21, 150])\n",
      "torch.Size([1024, 22, 150])\n",
      "torch.Size([1024, 20, 150])\n",
      "torch.Size([1024, 22, 150])\n",
      "torch.Size([704, 20, 150])\n",
      "Test Accuracy on Clickbait Dataset using RNN  : 0.956\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "with torch.no_grad():\n",
    "  Y_truth, Y_preds = [],[]\n",
    "  for X, X_len, Y in test_loader:\n",
    "    X = X.to(device)\n",
    "    outputs = model(X, X_len)\n",
    "\n",
    "    Y_truth.append(Y)\n",
    "    Y_preds.append(outputs)\n",
    "\n",
    "  Y_truth = torch.cat(Y_truth)\n",
    "  Y_preds = torch.cat(Y_preds)\n",
    "\n",
    "print(\"Test Accuracy on Clickbait Dataset using RNN  : {:.3f}\".format(accuracy_score(Y_truth.cpu().detach().numpy(), F.softmax(Y_preds, dim=-1).argmax(dim=-1).cpu().detach().numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i7JXqy4Yzj7R",
   "metadata": {
    "id": "i7JXqy4Yzj7R"
   },
   "source": [
    "Run the below cell to save the predictions. You will be required to upload the predictions on gradescope for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "YIoYfQJtzkdX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "YIoYfQJtzkdX",
    "outputId": "81b2dfae-81e3-4576-d8e9-58e6e6a0647a"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "preds = F.softmax(Y_preds, dim=-1).argmax(dim=-1).cpu().detach().numpy()\n",
    "\n",
    "with open('rnn_clickbait.pkl', 'wb') as fp:\n",
    "    pickle.dump(preds, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_sW4Am9kvdGA",
   "metadata": {
    "id": "_sW4Am9kvdGA"
   },
   "source": [
    "## 3.3 : Classifying Web of Science Dataset using RNN [No Points]\n",
    "\n",
    "Run the below cell to classify the WoS train and test dataset using the rnn functions that you have already implemented in 3.\n",
    "\n",
    "An accuracy of more than 35% is acceptable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "DPZE5g1uRI_I",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "DPZE5g1uRI_I",
    "outputId": "5b47c0ad-5e17-4424-c482-2d102e746fbe"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "max_words = 0\n",
    "for t in x_train_wos:\n",
    "    max_words = max(max_words, len(vocab_wos(tokenizer(t))))\n",
    "\n",
    "def vectorize_batch(batch):\n",
    "    Y, X = list(zip(*batch))\n",
    "    X = [vocab_wos(tokenizer(text)) for text in X] ## Tokenize and map tokens to indexes\n",
    "    X_len = [len(text) for text in X]\n",
    "    X = [tokens+([0]* (max_words-len(tokens))) if len(tokens)<max_words else tokens[:max_words] for tokens in X] \n",
    "    return torch.tensor(X, dtype=torch.int32), torch.tensor(X_len), torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "1ypsgOIIRUFm",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "1ypsgOIIRUFm",
    "outputId": "6c795bf1-88db-46e4-ec3e-606d7aaf0dfb"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "train_dataset = list(map(lambda y, x: (y, x), y_train_wos, x_train_wos))\n",
    "test_dataset = list(map(lambda y, x: (y, x), y_test_wos, x_test_wos))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=vectorize_batch, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=128, collate_fn=vectorize_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "195yI5vSvhWr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "195yI5vSvhWr",
    "outputId": "1b605091-1ba2-4735-df9d-3f53477f8403"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:50<00:00,  3.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 0: 17.797240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:59<00:00,  4.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 1: 16.947120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [01:02<00:00,  4.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 2: 16.422907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [01:06<00:00,  5.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 3: 15.834189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:56<00:00,  4.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 4: 15.368607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:49<00:00,  3.78s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 5: 14.741068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:48<00:00,  3.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 6: 14.151179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:51<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 7: 13.562327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:51<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 8: 13.051048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:45<00:00,  3.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 9: 12.492842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:48<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 10: 11.870239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:46<00:00,  3.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 11: 11.135075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:48<00:00,  3.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 12: 10.194510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:44<00:00,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 13: 9.688906\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:45<00:00,  3.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 14: 9.079505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:45<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 15: 8.269086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:45<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 16: 7.496048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:42<00:00,  3.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 17: 7.187210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:45<00:00,  3.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 18: 6.576881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 13/13 [00:47<00:00,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss on epoch 19: 5.942134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from rnn import RNN\n",
    "from tqdm import tqdm\n",
    "\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "model = RNN(vocab_wos, num_classes=NUM_CLASSES)\n",
    "model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "N_EPOCHS = 20\n",
    "\n",
    "model.train()\n",
    "for epoch in range(N_EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    for X, X_len, Y in tqdm(train_loader):\n",
    "      X = X.to(device)\n",
    "      Y = Y.to(device)\n",
    "      outputs = model(X, X_len)\n",
    "      loss = criterion(outputs, Y)\n",
    "\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "\n",
    "      total_loss += loss.item()\n",
    "\n",
    "    print(\"loss on epoch %i: %f\" % (epoch, total_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "ltNFDxdyvt-k",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ltNFDxdyvt-k",
    "outputId": "f9f407e2-4540-4955-c8db-b4b5543e1eb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy on WoS Dataset using RNN  : 0.405\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "with torch.no_grad():\n",
    "  Y_truth, Y_preds = [],[]\n",
    "  for X, X_len, Y in test_loader:\n",
    "    X = X.to(device)\n",
    "    outputs = model(X, X_len)\n",
    "\n",
    "    Y_truth.append(Y)\n",
    "    Y_preds.append(outputs)\n",
    "\n",
    "  Y_truth = torch.cat(Y_truth)\n",
    "  Y_preds = torch.cat(Y_preds)\n",
    "\n",
    "print(\"Test Accuracy on WoS Dataset using RNN  : {:.3f}\".format(accuracy_score(Y_truth.cpu().detach().numpy(), F.softmax(Y_preds, dim=-1).argmax(dim=-1).cpu().detach().numpy())))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CFfxVGC30Irh",
   "metadata": {
    "id": "CFfxVGC30Irh"
   },
   "source": [
    "Run the below cell to save the predictions. You will be required to upload the predictions on gradescope for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "ADcLlGMm0JOZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ADcLlGMm0JOZ",
    "outputId": "dc5cba1c-5269-466f-f48c-4ff0b5e9f05b"
   },
   "outputs": [],
   "source": [
    "###############################\n",
    "### DO NOT CHANGE THIS CELL ###\n",
    "###############################\n",
    "\n",
    "preds = F.softmax(Y_preds, dim=-1).argmax(dim=-1).cpu().detach().numpy()\n",
    "\n",
    "with open('rnn_wos.pkl', 'wb') as fp:\n",
    "    pickle.dump(preds, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Kb6rhadO0UEq",
   "metadata": {
    "id": "Kb6rhadO0UEq"
   },
   "source": [
    "**NOTE** : RNN alone is not able to perform good on the WoS dataset and that can be attributed to the very limited data with large vocabulary and lack of embedding structure."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "-Np3UUmtFUjd",
    "4TLUiFFR7wlL",
    "dRvybL9rYxcQ",
    "_sW4Am9kvdGA"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
